"""
å¤„ç†ä»»æ„æ–‡æœ¬ - å°†ä»»æ„ä¸€æ®µè¯è½¬æ¢æˆä¸‰ç§è®°å¿†ï¼ˆä½¿ç”¨åŠ¨æ€è®°å¿†ç³»ç»Ÿï¼‰
"""

import json
from llm_interface import create_llm
from memory_system import DynamicMemorySystem
from memory_storage import DynamicMemoryStorage


def process_any_text(text: str, output_file: str = "memory/output_from_text.json"):
    """
    å¤„ç†ä»»æ„æ–‡æœ¬ï¼Œæå–ä¸‰ç§è®°å¿†ç±»å‹ï¼ˆä½¿ç”¨åŠ¨æ€è®°å¿†ç³»ç»Ÿï¼‰
    
    Args:
        text: è¾“å…¥çš„æ–‡æœ¬
        output_file: è¾“å‡ºæ–‡ä»¶å
    """
    # åˆå§‹åŒ–è®¯é£æ˜Ÿç«LLMå’ŒåŠ¨æ€è®°å¿†ç³»ç»Ÿ
    llm = create_llm(
        provider="xinghuo",
        appid="75714447",
        api_key="79b6bd157e710cac51c22d357d182870",
        api_secret="NjUzMzNjYTE0MTBiODQ0NWVmZTliZDk5",
        api_version="v4.0",
        domain="4.0Ultra"
    )
    system = DynamicMemorySystem(llm)
    storage = DynamicMemoryStorage(system)
    
    # æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬å¹¶å­˜å‚¨
    import re
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', text)
    current_sentence = ""
    
    for part in sentences:
        current_sentence += part
        if part in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'] and current_sentence.strip():
            content = current_sentence.strip()
            if len(content) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„ç‰‡æ®µ
                system.store(content)
            current_sentence = ""
    
    # å¤„ç†æœ€åä¸€æ®µ
    if current_sentence.strip() and len(current_sentence.strip()) > 5:
        system.store(current_sentence.strip())
    
    # ç”Ÿæˆè¾“å‡º
    stats = system.get_statistics()
    output_data = {
        "input_text": text,
        "episodic_memories": [],
        "semantic_memories": [],
        "procedural_memories": [],
        "statistics": stats
    }
    
    # æŒ‰ç±»å‹æ”¶é›†è®°å¿†
    for memory in system.memories:
        mem_type = memory.get("type", "unknown")
        mem_entry = {
            "id": memory.get("id"),
            "content": memory.get("content"),
            "confidence": memory.get("confidence", 0.5),
            "importance": system.importance_scores.get(memory["id"], 0.5),
            "identification_method": "llm_classification",
            "extracted_info": memory.get("extracted_info", {})
        }
        
        if mem_type == "episodic":
            output_data["episodic_memories"].append(mem_entry)
        elif mem_type == "semantic":
            output_data["semantic_memories"].append(mem_entry)
        elif mem_type == "procedural":
            output_data["procedural_memories"].append(mem_entry)
    
    # ä¿å­˜è¾“å‡º
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æƒ…æ™¯è®°å¿†: {output_data['statistics']['episodic']} æ¡")
    print(f"   è¯­ä¹‰è®°å¿†: {output_data['statistics']['semantic']} æ¡")
    print(f"   ç¨‹åºè®°å¿†: {output_data['statistics']['procedural']} æ¡")
    print(f"   æ€»è®¡: {output_data['statistics']['total']} æ¡")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return output_data


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¤„ç†ä»»æ„æ–‡æœ¬
    sample_text = """
    ä»Šå¤©æ—©ä¸Š8ç‚¹ï¼Œæˆ‘åœ¨åŠå…¬å®¤å’ŒåŒäº‹è®¨è®ºäº†ä¸€ä¸ªæ–°é¡¹ç›®ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚
    å¦‚ä½•å‡†å¤‡ä¸€ä¸ªä¼šè®®ï¼šé¦–å…ˆç¡®å®šä¼šè®®ä¸»é¢˜å’Œç›®æ ‡ï¼Œç„¶åé‚€è¯·ç›¸å…³å‚ä¸è€…ï¼Œå‡†å¤‡ä¼šè®®ææ–™ï¼Œæœ€åå®‰æ’ä¼šè®®æ—¶é—´å’Œåœ°ç‚¹ã€‚
    æ˜¨å¤©ä¸‹åˆ3ç‚¹ï¼Œæˆ‘åœ¨å›¾ä¹¦é¦†é˜…è¯»äº†ä¸€æœ¬å…³äºæ·±åº¦å­¦ä¹ çš„ä¹¦ç±ï¼Œå­¦åˆ°äº†å¾ˆå¤šæ–°çŸ¥è¯†ã€‚ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¶æ„ã€‚
    """
    
    print("=" * 60)
    print("å¤„ç†ä»»æ„æ–‡æœ¬ç¤ºä¾‹")
    print("=" * 60)
    print(f"\nè¾“å…¥æ–‡æœ¬:\n{sample_text}\n")
    
    result = process_any_text(sample_text.strip())
    
    print("\n" + "=" * 60)
    print("æå–çš„è®°å¿†è¯¦æƒ…")
    print("=" * 60)
    
    if result["episodic_memories"]:
        print("\nã€æƒ…æ™¯è®°å¿†ã€‘")
        for mem in result["episodic_memories"]:
            print(f"  - {mem['content']}")
    
    if result["semantic_memories"]:
        print("\nã€è¯­ä¹‰è®°å¿†ã€‘")
        for mem in result["semantic_memories"]:
            print(f"  - {mem['content']}")
    
    if result["procedural_memories"]:
        print("\nã€ç¨‹åºè®°å¿†ã€‘")
        for mem in result["procedural_memories"]:
            print(f"  - {mem['content']}")
            if mem.get('extracted_steps'):
                print("    æ­¥éª¤:")
                for i, step in enumerate(mem['extracted_steps'], 1):
                    print(f"      {i}. {step}")
